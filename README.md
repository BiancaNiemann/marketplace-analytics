# ğŸ“˜ Marketplace Analytics Pipeline

A beginner-friendly guide to building an end-to-end data engineering project using modern tools and best practices.

## ğŸ¯ Project Overview

This project demonstrates a complete modern data engineering workflow that simulates a real marketplace environment. The pipeline generates synthetic marketplace data, stores it in Google BigQuery, transforms it using dbt, and orchestrates everything with Apache Airflowâ€”all running in Docker containers.

**What this pipeline does:**
- ğŸ§ª Generate fake marketplace data (clicks, orders, products, etc.)
- ğŸ“¦ Store raw data in BigQuery
- ğŸ”„ Transform data using dbt Core
- ğŸŒ¬ï¸ Orchestrate workflows with Airflow
- ğŸ—ï¸ Deploy infrastructure using Terraform
- ğŸ³ Run everything inside Docker containers

---

## ğŸ› ï¸ Tech Stack

| Tool | Purpose |
|------|---------|
| **Python + Faker** | Generate realistic synthetic marketplace data |
| **Google BigQuery** | Cloud data warehouse for storage and analytics |
| **dbt Core** | SQL-based data transformations and modeling |
| **Apache Airflow** | Workflow orchestration and scheduling |
| **Terraform** | Infrastructure as Code (IaC) for cloud resources |
| **Docker** | Containerization for consistent environments |

---

## ğŸ—ºï¸ Architecture



**Data Flow:**
```
Python + Faker â†’ CSV files â†’ BigQuery (raw) â†’ dbt (transform) â†’ BigQuery (analytics) â†’ Airflow (orchestration)
```

---

## ğŸ“‚ Project Structure

```
marketplace-analytics/
â”‚
â”œâ”€â”€ marketplace_dbt/                    # dbt project
â”‚   â”œâ”€â”€ models/                        # SQL transformation models
â”‚   â”‚   â”œâ”€â”€ staging/                   # Staging layer (stg_*)
â”‚   â”‚   â””â”€â”€ analytics/                 # Analytics layer
â”‚   â”œâ”€â”€ snapshots/                     # dbt snapshots
â”‚   â”œâ”€â”€ seeds/                         # Static CSV data
â”‚   â”œâ”€â”€ macros/                        # Reusable SQL macros
â”‚   â”œâ”€â”€ tests/                         # Data quality tests
â”‚   â”œâ”€â”€ dbt_project.yml               # dbt configuration
â”‚   â””â”€â”€ logs/                          # dbt logs
â”‚
â”œâ”€â”€ data_generator_script_and_files/   # Data generation
â”‚   â”œâ”€â”€ generate_data.py              # Python Faker scripts
â”‚   â””â”€â”€ *.csv                          # Generated CSV files
â”‚
â”œâ”€â”€ orchestration/                     # Airflow setup
â”‚   â””â”€â”€ airflow/
â”‚       â”œâ”€â”€ dags/
â”‚       â”‚   â””â”€â”€ dbt_pipeline.py       # Main DAG
â”‚       â”œâ”€â”€ dbt_profiles/
â”‚       â”‚   â””â”€â”€ profiles.yml          # dbt BigQuery connection
â”‚       â”œâ”€â”€ docker-compose.yml        # Airflow services
â”‚       â””â”€â”€ Dockerfile                # Custom Airflow image
â”‚
â”œâ”€â”€ infrastructure/                    # Terraform IaC
â”‚   â””â”€â”€ terraform/
â”‚       â”œâ”€â”€ main.tf                   # Main infrastructure config
â”‚       â”œâ”€â”€ variables.tf              # Input variables
â”‚       â””â”€â”€ outputs.tf                # Output values
â”‚
â”œâ”€â”€ .gitignore                        # Git ignore rules
â””â”€â”€ README.md                         # This file
```

---

## ğŸš€ Getting Started

### Prerequisites

- **Docker** and **Docker Compose** installed
- **Google Cloud Platform** account
- **Terraform** installed (for infrastructure setup)
- **Python 3.8+** (for data generation)

### 1. Clone the Repository

```bash
git clone https://github.com/BiancaNiemann/marketplace-analytics.git
cd marketplace-analytics
```

### 2. Set Up Google Cloud Credentials

1. Create a service account in Google Cloud Console
2. Download the JSON key file
3. Store it securely on your local machine:

```bash
mkdir -p ~/.keys
mv ~/Downloads/your-service-account-key.json ~/.keys/service-account-key.json
chmod 600 ~/.keys/service-account-key.json
```

### 3. Configure dbt Profiles

Update `orchestration/airflow/dbt_profiles/profiles.yml` with your project details:

```yaml
marketplace_dbt:
  target: dev
  outputs:
    dev:
      type: bigquery
      method: service-account
      project: your-gcp-project-id        # Update this
      dataset: analytics                   # Update if needed
      threads: 4
      keyfile: /opt/airflow/.dbt/service-account-key.json
      location: EU                         # or US
```

### 4. Deploy Infrastructure with Terraform

```bash
cd infrastructure/terraform
terraform init
terraform plan
terraform apply
```

This creates:
- BigQuery datasets (raw and analytics)
- Service accounts
- IAM permissions

### 5. Start Airflow

```bash
cd orchestration/airflow
docker-compose up -d --build
```

### 6. Create Airflow Admin User

```bash
docker-compose run airflow-webserver airflow users create \
  --username admin \
  --firstname Admin \
  --lastname User \
  --role Admin \
  --email admin@example.com \
  --password admin
```

### 7. Access Airflow UI

Open your browser and navigate to:
```
http://localhost:8080
```

Login with:
- **Username:** `admin`
- **Password:** `admin`

### 8. Trigger the DAG

In the Airflow UI:
1. Find the `dbt_bigquery_pipeline` DAG
2. Toggle it to "On"
3. Click the play button to trigger manually

---

## ğŸ”„ Pipeline Workflow

The Airflow DAG orchestrates the following tasks:

1. **`dbt_deps`** - Install dbt package dependencies
2. **`dbt_run`** - Execute dbt models to transform raw data
3. **`dbt_test`** - Run data quality tests
4. **`load_csv_to_bigquery`** - Load raw CSV files into BigQuery

### Task Dependencies

```
dbt_deps â†’ dbt_run â†’ dbt_test
         â†˜ load_csv_to_bigquery
```

---

## ğŸ§ª Data Generation

Generate synthetic marketplace data using Python and Faker:

```bash
cd data_generator_script_and_files
python generate_data.py
```

This creates CSV files with:
- User data
- Product catalog
- Click events
- Order transactions
- Reviews

---

## ğŸ§  dbt Models

### Staging Layer (`models/staging/`)
Clean and standardize raw data:
- `stg_clicks.sql`
- `stg_orders.sql`
- `stg_products.sql`
- `stg_users.sql`

### Analytics Layer (`models/analytics/`)
Business logic and metrics:
- `fct_orders.sql` - Order facts
- `dim_products.sql` - Product dimensions
- `dim_users.sql` - User dimensions

### Run dbt Locally

```bash
cd marketplace_dbt
dbt run
dbt test
dbt docs generate
dbt docs serve
```

---

## ğŸ³ Docker Services

The `docker-compose.yml` file defines:

| Service | Purpose | Port |
|---------|---------|------|
| **postgres** | Airflow metadata database | 5432 |
| **airflow-webserver** | Airflow UI | 8080 |
| **airflow-scheduler** | Task scheduling and execution | - |

### Useful Docker Commands

```bash
# Start all services
docker-compose up -d

# Stop all services
docker-compose down

# View logs
docker-compose logs -f airflow-scheduler

# Rebuild containers
docker-compose up -d --build

# Execute commands inside container
docker-compose exec airflow-webserver bash
```

---

## ğŸ§± Infrastructure as Code

Terraform manages:
- **BigQuery datasets** (raw, analytics)
- **Service accounts** with appropriate permissions
- **IAM roles** for secure access

### Terraform Commands

```bash
cd infrastructure/terraform

# Initialize Terraform
terraform init

# Preview changes
terraform plan

# Apply changes
terraform apply

# Destroy infrastructure
terraform destroy
```

---

## ğŸ“ What You'll Learn

Building this project teaches you:

âœ… **Data Generation** - Create realistic synthetic datasets with Faker  
âœ… **Cloud Data Warehousing** - Work with Google BigQuery  
âœ… **Data Transformation** - Build dbt models with SQL  
âœ… **Workflow Orchestration** - Schedule and monitor with Airflow  
âœ… **Containerization** - Deploy with Docker and Docker Compose  
âœ… **Infrastructure as Code** - Automate cloud resources with Terraform  
âœ… **Best Practices** - Version control, testing, documentation  

---

## ğŸ“Š Monitoring and Logs

### Airflow Logs
- **Location:** `orchestration/airflow/logs/`
- **View in UI:** Click on any task in the Airflow UI to see logs

### dbt Logs
- **Location:** `marketplace_dbt/logs/dbt.log`
- **Run logs:** Generated after each `dbt run`

### BigQuery
Monitor queries and costs in the [GCP Console](https://console.cloud.google.com/bigquery)

---

## ğŸ”’ Security Best Practices

- âœ… **Never commit credentials** - Use `.gitignore` to exclude keys
- âœ… **Use service accounts** - Follow principle of least privilege
- âœ… **Rotate keys regularly** - Update service account keys periodically
- âœ… **Store secrets securely** - Keep keys in `~/.keys/` locally
- âœ… **Use environment variables** - For sensitive configuration

---

## ğŸ› Troubleshooting

### Common Issues

**1. "No such file or directory" for service account key**
- Ensure the key is at `~/.keys/service-account-key.json`
- Check docker-compose volume mount
- Verify profiles.yml keyfile path

**2. Airflow tasks failing**
- Check task logs in Airflow UI
- Verify BigQuery permissions
- Ensure dbt models are valid SQL

**3. dbt connection errors**
- Validate profiles.yml configuration
- Test service account permissions in GCP Console
- Check BigQuery dataset exists

**4. Docker containers not starting**
- Run `docker-compose logs` to see errors
- Ensure ports 8080 and 5432 are available
- Try `docker-compose down && docker-compose up --build`

---

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“ License

This project is open source and available for educational purposes.

---

## ğŸ“§ Contact

**Bianca Niemann**  
GitHub: [@BiancaNiemann](https://github.com/BiancaNiemann)

---

## ğŸ™ Acknowledgments

- **Faker** - For synthetic data generation
- **dbt Labs** - For the amazing dbt framework
- **Apache Airflow** - For workflow orchestration
- **Google Cloud Platform** - For BigQuery and cloud infrastructure
- **Terraform** - For infrastructure as code capabilities

---

â­ **Star this repo** if you find it helpful!

